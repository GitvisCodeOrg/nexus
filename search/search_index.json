{
    "docs": [
        {
            "location": "/",
            "text": "Nexus\n\n\n\n\n\ud83d\udea7 \nOngoing project\n \ud83d\udea7 \nStatus: Prototype\n \ud83d\udea7\n\n\nNexus is a prototypical typesafe deep learning system in Scala.\n\n\nNexus is a departure from common deep learning libraries such as \nTensorFlow\n, \nPyTorch\n, \nTheano\n, \nMXNet\n, etc. \n\n\n\n\nEver been baffled by the axes of tensors? Which axis should I max out? \n\n\nEver got \nTypeError\ns in Python?\n\n\nEver spending hours or days getting the tensors' axes and dimensions right?\n\n\n\n\nNexus' answer to these problems is \nstatic types\n. By specifying tensor axes' semantics in  types exploiting Scala's expressive types, compilers can validate the program \nat compile time\n, freeing developers' burden of remembering axes by heart, and eliminating nearly all errors above before even running.\n\n\nNexus embraces \ndeclarative\n and \nfunctional\n programming: Neural networks are built using small composable components, making code very easy to follow, understand and maintain.\n\n\nA first glance\n\n\nA simple neural network for learning the XOR function can be found \nhere\n.\n\n\nBuilding a typesafe XOR network:\n\n  \nclass\n \nIn\n;\n     \nval\n \nIn\n \n=\n \nnew\n \nIn\n          \n  \nclass\n \nHidden\n;\n \nval\n \nHidden\n \n=\n \nnew\n \nHidden\n\n  \nclass\n \nOut\n;\n    \nval\n \nOut\n \n=\n \nnew\n \nOut\n      \n// tensor axis labels declared as types and singletons\n\n\n  \nval\n \nx\n \n=\n \nInput\n[\nFloatTensor\n[\nIn\n]]()\n     \n// input vectors\n\n  \nval\n \ny\n \n=\n \nInput\n[\nFloatTensor\n[\nOut\n]]()\n    \n// gold labels\n\n\n \u00a0\nval\n \n\u0177\n \n=\n \nx\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0  \u00a0 \u00a0 \u00a0 \u00a0  \u00a0 \n|>\n   \n// type: Expr[FloatTensor[In]]\n\n    \nAffine\n(\nIn\n \n->\n \n2\n,\n \nHidden\n \n->\n \n2\n)\n  \n|>\n   \n// type: Expr[FloatTensor[Hidden]]\n\n    \nSigmoid\n                       \n|>\n   \n// type: Expr[FloatTensor[Hidden]]\n\n    \nAffine\n(\nHidden\n \n->\n \n2\n,\n \nOut\n \n->\n \n2\n)\n \n|>\n   \n// type: Expr[FloatTensor[Out]]\n\n    \nSoftmax\n                            \n// type: Expr[FloatTensor[Out]]\n\n \u00a0\nval\n \nloss\n \n=\n \nCrossEntropy\n(\ny\n,\n \n\u0177\n)\n      \u00a0 \n// type: Expr[Float]\n\n\n\n\nDesign goals\n\n\n\n\nTypeful\n. Each axis of a tensor is statically typed using tuples. For example, an image is typed as \nFloatTensor[(Width, Height, Channel)]\n, whereas an embedded sentence is typed as \nFloatTensor[(Word, Embedding)]\n. This frees programmers from remembering what each axis stands for.\n\n\nTypesafe\n.  Very strong static type checking to eliminate most bugs at compile time.\n\n\nNever, ever specify axis index again\n. For things like \nreduce_sum(x, axis=1)\n, write \nx |> SumAlong(AxisName)\n.\n\n\nMixing differentiable code with non-differentiable code\n.\n\n\nAutomatic typeclass derivation\n: Differentiation through any case class (product type).\n\n\nVersatile switching between eager and lazy evaluation\n.\n\n\n[TODO] Typesafe tensor sizes using literal singleton types (Scala 2.13+)\n. \n\n\n[TODO] Automatic batching over sequences/trees\n (Neubig, Goldberg, Dyer, NIPS 2017). Free programmers from the pain of manual batching.\n\n\n[TODO] GPU Acceleration\n. Reuse \nTorch\n C++ core through Swig \n(bindings)\n.\n\n\n[TODO] Multiple backends\n. Torch / MXNet / ?\n\n\n[TODO] Automatic operator fusion for optimization.\n\n\n[TODO] Typesafe higher-order gradients\n.\n\n\n\n\nCitation\n\n\nPlease cite this in academic work as\n\n\n\n\nTongfei Chen (2017): \nTypesafe Abstractions for Tensor Operations\n. In \nProceedings of the 8\nth\n ACM SIGPLAN International Symposium on Scala\n, pp. 45-50.\n\n\n\n\n@inproceedings\n{\nchen2017typesafe\n,\n\n \nauthor\n \n=\n \n{Chen, Tongfei}\n,\n\n \ntitle\n \n=\n \n{Typesafe Abstractions for Tensor Operations (Short Paper)}\n,\n\n \nbooktitle\n \n=\n \n{Proceedings of the 8th ACM SIGPLAN International Symposium on Scala}\n,\n\n \nseries\n \n=\n \n{SCALA 2017}\n,\n\n \nyear\n \n=\n \n{2017}\n,\n\n \npages\n \n=\n \n{45--50}\n,\n\n \nurl\n \n=\n \n{http://doi.acm.org/10.1145/3136000.3136001}\n,\n\n \ndoi\n \n=\n \n{10.1145/3136000.3136001}\n\n\n}",
            "title": "Introduction"
        },
        {
            "location": "/#nexus",
            "text": "\ud83d\udea7  Ongoing project  \ud83d\udea7  Status: Prototype  \ud83d\udea7  Nexus is a prototypical typesafe deep learning system in Scala.  Nexus is a departure from common deep learning libraries such as  TensorFlow ,  PyTorch ,  Theano ,  MXNet , etc.    Ever been baffled by the axes of tensors? Which axis should I max out?   Ever got  TypeError s in Python?  Ever spending hours or days getting the tensors' axes and dimensions right?   Nexus' answer to these problems is  static types . By specifying tensor axes' semantics in  types exploiting Scala's expressive types, compilers can validate the program  at compile time , freeing developers' burden of remembering axes by heart, and eliminating nearly all errors above before even running.  Nexus embraces  declarative  and  functional  programming: Neural networks are built using small composable components, making code very easy to follow, understand and maintain.",
            "title": "Nexus"
        },
        {
            "location": "/#a-first-glance",
            "text": "A simple neural network for learning the XOR function can be found  here .  Building a typesafe XOR network:    class   In ;       val   In   =   new   In           \n   class   Hidden ;   val   Hidden   =   new   Hidden \n   class   Out ;      val   Out   =   new   Out        // tensor axis labels declared as types and singletons \n\n   val   x   =   Input [ FloatTensor [ In ]]()       // input vectors \n   val   y   =   Input [ FloatTensor [ Out ]]()      // gold labels \n\n \u00a0 val   \u0177   =   x  \u00a0 \u00a0 \u00a0 \u00a0 \u00a0  \u00a0 \u00a0 \u00a0 \u00a0  \u00a0  |>     // type: Expr[FloatTensor[In]] \n     Affine ( In   ->   2 ,   Hidden   ->   2 )    |>     // type: Expr[FloatTensor[Hidden]] \n     Sigmoid                         |>     // type: Expr[FloatTensor[Hidden]] \n     Affine ( Hidden   ->   2 ,   Out   ->   2 )   |>     // type: Expr[FloatTensor[Out]] \n     Softmax                              // type: Expr[FloatTensor[Out]] \n \u00a0 val   loss   =   CrossEntropy ( y ,   \u0177 )       \u00a0  // type: Expr[Float]",
            "title": "A first glance"
        },
        {
            "location": "/#design-goals",
            "text": "Typeful . Each axis of a tensor is statically typed using tuples. For example, an image is typed as  FloatTensor[(Width, Height, Channel)] , whereas an embedded sentence is typed as  FloatTensor[(Word, Embedding)] . This frees programmers from remembering what each axis stands for.  Typesafe .  Very strong static type checking to eliminate most bugs at compile time.  Never, ever specify axis index again . For things like  reduce_sum(x, axis=1) , write  x |> SumAlong(AxisName) .  Mixing differentiable code with non-differentiable code .  Automatic typeclass derivation : Differentiation through any case class (product type).  Versatile switching between eager and lazy evaluation .  [TODO] Typesafe tensor sizes using literal singleton types (Scala 2.13+) .   [TODO] Automatic batching over sequences/trees  (Neubig, Goldberg, Dyer, NIPS 2017). Free programmers from the pain of manual batching.  [TODO] GPU Acceleration . Reuse  Torch  C++ core through Swig  (bindings) .  [TODO] Multiple backends . Torch / MXNet / ?  [TODO] Automatic operator fusion for optimization.  [TODO] Typesafe higher-order gradients .",
            "title": "Design goals"
        },
        {
            "location": "/#citation",
            "text": "Please cite this in academic work as   Tongfei Chen (2017):  Typesafe Abstractions for Tensor Operations . In  Proceedings of the 8 th  ACM SIGPLAN International Symposium on Scala , pp. 45-50.   @inproceedings { chen2017typesafe , \n  author   =   {Chen, Tongfei} , \n  title   =   {Typesafe Abstractions for Tensor Operations (Short Paper)} , \n  booktitle   =   {Proceedings of the 8th ACM SIGPLAN International Symposium on Scala} , \n  series   =   {SCALA 2017} , \n  year   =   {2017} , \n  pages   =   {45--50} , \n  url   =   {http://doi.acm.org/10.1145/3136000.3136001} , \n  doi   =   {10.1145/3136000.3136001}  }",
            "title": "Citation"
        },
        {
            "location": "/tensor/",
            "text": "Typesafe Tensors\n\n\nThe distinctive feature of Nexus is \ntypesafe\n tensors.\n\n\nIn Nexus, a tensor is typed using a tuple of axis labels.\n\n\nval\n \nimage\n:\n \nFloatTensor\n[(\nWidth\n, \nHeight\n, \nChannel\n)]\n\n\n\n\n\nThis is in contrast with common deep learning libraries, where multidimensional arrays (tensors) all belong to one type (given the element type). For example, a tensor with float is typed as \nnumpy.ndarray\n in NumPy and \ntorch.FloatTensor\n in PyTorch, no matter how many dimensions are there (rank) in the tensor, or what each axis means in the tensor.",
            "title": "Typesafe tensors"
        },
        {
            "location": "/tensor/#typesafe-tensors",
            "text": "The distinctive feature of Nexus is  typesafe  tensors.  In Nexus, a tensor is typed using a tuple of axis labels.  val   image :   FloatTensor [( Width ,  Height ,  Channel )]   This is in contrast with common deep learning libraries, where multidimensional arrays (tensors) all belong to one type (given the element type). For example, a tensor with float is typed as  numpy.ndarray  in NumPy and  torch.FloatTensor  in PyTorch, no matter how many dimensions are there (rank) in the tensor, or what each axis means in the tensor.",
            "title": "Typesafe Tensors"
        },
        {
            "location": "/expr/",
            "text": "Expressions\n\n\nThe core data type in Nexus is \nExpr[A]\n: expressions. An \nExpr[A]\n is a symbolic expression that conceptually holds data of type \nA\n.\n\n\nAnalogous types in other libraries\nExpr\n is similar to\ntf.Tensor\n: \ntorch.autograd.Variable\n (before PyTorch 0.4.0): \nAn expression \nExpr[A]\n in Nexus can be one of the following:\n\n\n\n\nInput[A]\n: Symbolic input of type \nA\n.\n\n\nConst[A]\n: Constant.\n\n\nParam[A]\n: A parameter that will be updated while training.\n\n\nAppN[A]\n (\nN\n = {1, 2, 3}): Result of applying a \nN\nN\n-ary function.",
            "title": "Expressions"
        },
        {
            "location": "/expr/#expressions",
            "text": "The core data type in Nexus is  Expr[A] : expressions. An  Expr[A]  is a symbolic expression that conceptually holds data of type  A .  Analogous types in other libraries Expr  is similar to tf.Tensor :  torch.autograd.Variable  (before PyTorch 0.4.0):  An expression  Expr[A]  in Nexus can be one of the following:   Input[A] : Symbolic input of type  A .  Const[A] : Constant.  Param[A] : A parameter that will be updated while training.  AppN[A]  ( N  = {1, 2, 3}): Result of applying a  N N -ary function.",
            "title": "Expressions"
        },
        {
            "location": "/op/",
            "text": "Operators\n\n\nIn Nexus operators can be \"grounded\" or polymorphic.\n\n\n[A, B], [B, C] \\to [A, C]\n[A, B], [B, C] \\to [A, C]",
            "title": "Operators"
        },
        {
            "location": "/op/#operators",
            "text": "In Nexus operators can be \"grounded\" or polymorphic.  [A, B], [B, C] \\to [A, C] [A, B], [B, C] \\to [A, C]",
            "title": "Operators"
        }
    ]
}