{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Nexus \ud83d\udea7 Ongoing project \ud83d\udea7 Status: Prototype \ud83d\udea7 Nexus is a prototypical typesafe deep learning system in Scala. Nexus is a departure from common deep learning libraries such as TensorFlow , PyTorch , MXNet , etc. Ever been baffled by the axes of tensors? Which axis should I max out? Ever got TypeError s in Python? Ever spending hours or days getting the tensors' axes and dimensions right? Nexus' answer to these problems is static types . By specifying tensor axes' semantics in types exploiting Scala's expressive types, compilers can validate the program at compile time , freeing developers' burden of remembering axes by heart, and eliminating nearly all errors above before even running. Nexus embraces declarative and functional programming: Neural networks are built using small composable components, making code very easy to follow, understand and maintain. A first glance A simple neural network for learning the XOR function can be found here . Building a typesafe XOR network: class In extends Dim ; val In = new In class Hidden extends Dim ; val Hidden = new Hidden class Out extends Dim ; val Out = new Out // tensor axis labels declared as types and singletons val x = Input [ FloatTensor [ In ]]() // input vectors val y = Input [ FloatTensor [ Out ]]() // gold labels val \u0177 = x |> // type: Symbolic[FloatTensor[In]] Affine ( In -> 2 , Hidden -> 2 ) |> // type: Symbolic[FloatTensor[Hidden]] Sigmoid |> // type: Symbolic[FloatTensor[Hidden]] Affine ( Hidden -> 2 , Out -> 2 ) |> // type: Symbolic[FloatTensor[Out]] Softmax // type: Symbolic[FloatTensor[Out]] val loss = CrossEntropy ( y , \u0177 ) // type: Symbolic[Float] Design goals Typeful . Each axis of a tensor is statically typed using tuples. For example, an image is typed as FloatTensor[(Width, Height, Channel)] , whereas an embedded sentence is typed as FloatTensor[(Word, Embedding)] . This frees programmers from remembering what each axis stands for. Typesafe . Very strong static type checking to eliminate most bugs at compile time. Never, ever specify axis index again . For things like reduce_sum(x, axis=1) , write x |> SumAlong(AxisName) . Mixing differentiable code with non-differentiable code . Automatic typeclass derivation : Differentiation through any case class (product type). Versatile switching between eager and lazy evaluation . [TODO] Typesafe tensor sizes using literal singleton types (Scala 2.13+) . [TODO] Automatic batching over sequences/trees (Neubig, Goldberg, Dyer, NIPS 2017). Free programmers from the pain of manual batching. [TODO] GPU Acceleration . Reuse Torch C++ core through Swig (bindings) . [TODO] Multiple backends . Torch / MXNet? / TensorFlow? / TensorFlow.js for Scala.js? / Lib [TODO] Automatic operator fusion for optimization. [TODO] Typesafe higher-order gradients . Modules Nexus is modularized. It contains the following modules: Module Description nexus-tensor Foundations for typesafe tensors nexus-prob Typesafe probabilistic programming nexus-diff Typesafe deep learning (differentiable programming) nexus-ml High-level machine learning abstractions / models nexus-jvm-backend JVM reference backend (slow) nexus-torch Torch native CPU backend nexus-torch-cuda Torch CUDA GPU backend Citation Please cite this in academic work as Tongfei Chen (2017): Typesafe Abstractions for Tensor Operations . In Proceedings of the 8 th ACM SIGPLAN International Symposium on Scala , pp. 45-50. @inproceedings { chen2017typesafe , author = {Chen, Tongfei} , title = {Typesafe Abstractions for Tensor Operations (Short Paper)} , booktitle = {Proceedings of the 8th ACM SIGPLAN International Symposium on Scala} , series = {SCALA 2017} , year = {2017} , pages = {45--50} , url = {http://doi.acm.org/10.1145/3136000.3136001} , doi = {10.1145/3136000.3136001} }","title":"Introduction"},{"location":"#nexus","text":"\ud83d\udea7 Ongoing project \ud83d\udea7 Status: Prototype \ud83d\udea7 Nexus is a prototypical typesafe deep learning system in Scala. Nexus is a departure from common deep learning libraries such as TensorFlow , PyTorch , MXNet , etc. Ever been baffled by the axes of tensors? Which axis should I max out? Ever got TypeError s in Python? Ever spending hours or days getting the tensors' axes and dimensions right? Nexus' answer to these problems is static types . By specifying tensor axes' semantics in types exploiting Scala's expressive types, compilers can validate the program at compile time , freeing developers' burden of remembering axes by heart, and eliminating nearly all errors above before even running. Nexus embraces declarative and functional programming: Neural networks are built using small composable components, making code very easy to follow, understand and maintain.","title":"Nexus"},{"location":"#a-first-glance","text":"A simple neural network for learning the XOR function can be found here . Building a typesafe XOR network: class In extends Dim ; val In = new In class Hidden extends Dim ; val Hidden = new Hidden class Out extends Dim ; val Out = new Out // tensor axis labels declared as types and singletons val x = Input [ FloatTensor [ In ]]() // input vectors val y = Input [ FloatTensor [ Out ]]() // gold labels val \u0177 = x |> // type: Symbolic[FloatTensor[In]] Affine ( In -> 2 , Hidden -> 2 ) |> // type: Symbolic[FloatTensor[Hidden]] Sigmoid |> // type: Symbolic[FloatTensor[Hidden]] Affine ( Hidden -> 2 , Out -> 2 ) |> // type: Symbolic[FloatTensor[Out]] Softmax // type: Symbolic[FloatTensor[Out]] val loss = CrossEntropy ( y , \u0177 ) // type: Symbolic[Float]","title":"A first glance"},{"location":"#design-goals","text":"Typeful . Each axis of a tensor is statically typed using tuples. For example, an image is typed as FloatTensor[(Width, Height, Channel)] , whereas an embedded sentence is typed as FloatTensor[(Word, Embedding)] . This frees programmers from remembering what each axis stands for. Typesafe . Very strong static type checking to eliminate most bugs at compile time. Never, ever specify axis index again . For things like reduce_sum(x, axis=1) , write x |> SumAlong(AxisName) . Mixing differentiable code with non-differentiable code . Automatic typeclass derivation : Differentiation through any case class (product type). Versatile switching between eager and lazy evaluation . [TODO] Typesafe tensor sizes using literal singleton types (Scala 2.13+) . [TODO] Automatic batching over sequences/trees (Neubig, Goldberg, Dyer, NIPS 2017). Free programmers from the pain of manual batching. [TODO] GPU Acceleration . Reuse Torch C++ core through Swig (bindings) . [TODO] Multiple backends . Torch / MXNet? / TensorFlow? / TensorFlow.js for Scala.js? / Lib [TODO] Automatic operator fusion for optimization. [TODO] Typesafe higher-order gradients .","title":"Design goals"},{"location":"#modules","text":"Nexus is modularized. It contains the following modules: Module Description nexus-tensor Foundations for typesafe tensors nexus-prob Typesafe probabilistic programming nexus-diff Typesafe deep learning (differentiable programming) nexus-ml High-level machine learning abstractions / models nexus-jvm-backend JVM reference backend (slow) nexus-torch Torch native CPU backend nexus-torch-cuda Torch CUDA GPU backend","title":"Modules"},{"location":"#citation","text":"Please cite this in academic work as Tongfei Chen (2017): Typesafe Abstractions for Tensor Operations . In Proceedings of the 8 th ACM SIGPLAN International Symposium on Scala , pp. 45-50. @inproceedings { chen2017typesafe , author = {Chen, Tongfei} , title = {Typesafe Abstractions for Tensor Operations (Short Paper)} , booktitle = {Proceedings of the 8th ACM SIGPLAN International Symposium on Scala} , series = {SCALA 2017} , year = {2017} , pages = {45--50} , url = {http://doi.acm.org/10.1145/3136000.3136001} , doi = {10.1145/3136000.3136001} }","title":"Citation"},{"location":"expr/","text":"Expressions The core data type in Nexus is Expr[A] : expressions. An Expr[A] is a symbolic expression that conceptually holds data of type A . Analogous types in other libraries Expr is similar to tf.Tensor : torch.autograd.Variable (before PyTorch 0.4.0): An expression Expr[A] in Nexus can be one of the following: Input[A] : Symbolic input of type A . Const[A] : Constant. Param[A] : A parameter that will be updated while training. AppN[A] ( N = {1, 2, 3}): Result of applying a N N -ary function.","title":"Expressions"},{"location":"expr/#expressions","text":"The core data type in Nexus is Expr[A] : expressions. An Expr[A] is a symbolic expression that conceptually holds data of type A . Analogous types in other libraries Expr is similar to tf.Tensor : torch.autograd.Variable (before PyTorch 0.4.0): An expression Expr[A] in Nexus can be one of the following: Input[A] : Symbolic input of type A . Const[A] : Constant. Param[A] : A parameter that will be updated while training. AppN[A] ( N = {1, 2, 3}): Result of applying a N N -ary function.","title":"Expressions"},{"location":"op/","text":"Operators In Nexus operators can be \"grounded\" or polymorphic. [A, B], [B, C] \\to [A, C] [A, B], [B, C] \\to [A, C]","title":"Operators"},{"location":"op/#operators","text":"In Nexus operators can be \"grounded\" or polymorphic. [A, B], [B, C] \\to [A, C] [A, B], [B, C] \\to [A, C]","title":"Operators"},{"location":"tensor/","text":"Typesafe Tensors The distinctive feature of Nexus is typesafe tensors. In Nexus, a tensor is typed using a tuple of axis labels. val image : FloatTensor [( Width , Height , Channel )] This is in contrast with common deep learning libraries, where multidimensional arrays (tensors) all belong to one type (given the element type). For example, a tensor with float is typed as numpy.ndarray in NumPy and torch.FloatTensor in PyTorch, no matter how many dimensions are there (rank) in the tensor, or what each axis means in the tensor.","title":"Typesafe tensors"},{"location":"tensor/#typesafe-tensors","text":"The distinctive feature of Nexus is typesafe tensors. In Nexus, a tensor is typed using a tuple of axis labels. val image : FloatTensor [( Width , Height , Channel )] This is in contrast with common deep learning libraries, where multidimensional arrays (tensors) all belong to one type (given the element type). For example, a tensor with float is typed as numpy.ndarray in NumPy and torch.FloatTensor in PyTorch, no matter how many dimensions are there (rank) in the tensor, or what each axis means in the tensor.","title":"Typesafe Tensors"},{"location":"torch/","text":"nexus-torch is a tensor computation backend for Nexus based on PyTorch . It uses SWIG to generate the JNI bindings directly from the Torch C/C++ source without the Python part of PyTorch. Tested environments Linux + CUDA 9.1 / 10.0 + PyTorch 1.0.0 Building the binding manually Prerequisites: PyTorch 1.0.0 pip install torch==1.0.0 SWIG 3.0+ Steps: Patch SWIG . Use the swig-patch/fix-long.patch file to patch SWIG: This resolves SWIG issue #646 , which incorrectly maps ptrdiff_t to Int instead of the correct Long under 64-bit machines. Generate JNI shared library : Run build.sh to build the shared library ( libjnitorch.so / libjnitorch.dylib ). Internals","title":"Torch"},{"location":"torch/#tested-environments","text":"Linux + CUDA 9.1 / 10.0 + PyTorch 1.0.0","title":"Tested environments"},{"location":"torch/#building-the-binding-manually","text":"Prerequisites: PyTorch 1.0.0 pip install torch==1.0.0 SWIG 3.0+ Steps: Patch SWIG . Use the swig-patch/fix-long.patch file to patch SWIG: This resolves SWIG issue #646 , which incorrectly maps ptrdiff_t to Int instead of the correct Long under 64-bit machines. Generate JNI shared library : Run build.sh to build the shared library ( libjnitorch.so / libjnitorch.dylib ).","title":"Building the binding manually"},{"location":"torch/#internals","text":"","title":"Internals"}]}